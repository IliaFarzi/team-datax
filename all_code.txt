### File: C:\Users\Asus\Desktop\team-datax\api\app\agent.py
# api/app/agent.py
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import StructuredTool
from langgraph.checkpoint.memory import MemorySaver 
from langchain_core.output_parsers import StrOutputParser

from fastapi import Request

import os
from dotenv import load_dotenv
import logging 

logger = logging.getLogger(__name__)

from api.app.sheet_tools import (
    list_google_sheets,
    preview_google_sheet,
    load_google_sheet_to_dataframe,
    analyze_google_sheet
)
from api.app.upload_router import analyze_uploaded_file, list_uploaded_files

from api.app.embeddings import embed_text

from api.app.vectorstore import search_vectors


def make_wrapped_tools(request: Request):

    user_id = str(request.session.get("user_id"))

    # RAG tool
    def wrapped_search_vector_db(query: str, top_k: int = 5):
        query_vector = embed_text([query])[0]
        results = search_vectors(user_id, query_vector, top_k=top_k)
        texts = [r.payload.get("chunk", "") for r in results]
        stitched = "\n\n".join(texts)
        logger.info("Using SearchVectorDB tool ًں”§")
        return stitched or "No relevant context found."
    
    # Show all data in one place
    def wrapped_show_all_data():
        logger.info("Using ShowAllData tool ًں”§")
        uploads = list_uploaded_files(user_id=user_id)
        sheets = list_google_sheets(user_id=user_id)

        if not uploads and not sheets:
            return "ظ‡غŒع† ظپط§غŒظ„غŒ غŒط§ ط´غŒطھغŒ ظ¾غŒط¯ط§ ظ†ط´ط¯."

        # ًں“Œ Markdown the output so it can be displayed directly to the user
        md = "### ًں“‚ ط¯ط§ط¯ظ‡â€Œظ‡ط§غŒ ط´ظ…ط§\n"
        if uploads:
            md += "\n**ظپط§غŒظ„â€Œظ‡ط§غŒ ط¢ظ¾ظ„ظˆط¯ ط´ط¯ظ‡:**\n"
            for f in uploads:
                md += f"- {f}\n"
        if sheets:
            md += "\n**ع¯ظˆع¯ظ„ ط´غŒطھâ€Œظ‡ط§:**\n"
            for s in sheets:
                md += f"- {s}\n"

        return md

    # Google Sheets tools
    def wrapped_list_google_sheets():
        logger.info("Using ListGoogleSheets tool ًں”§")
        return list_google_sheets(user_id=user_id)

    def wrapped_preview_google_sheet(sheet_id: str):
        logger.info("Using PreviewGoogleSheet tool ًں”§")
        return preview_google_sheet(sheet_id=sheet_id, user_id=user_id)

    def wrapped_load_google_sheet_to_dataframe(sheet_id: str):
        logger.info("Using LoadGoogleSheet tool ًں”§")
        return load_google_sheet_to_dataframe(sheet_id=sheet_id, user_id=user_id)

    def wrapped_analyze_google_sheet(sheet_id: str, operation: str, column: str, value: str = None):
        logger.info("Using AnalyzeGoogleSheet tool ًں”§")
        return analyze_google_sheet(
            sheet_id=sheet_id,
            user_id=user_id,
            operation=operation,
            column=column,
            value=value
        )

    # Upload tools
    def wrapped_list_uploaded_files():
        logger.info("Using ListUploadedFiles tool ًں”§")
        return list_uploaded_files(user_id=user_id)

    def wrapped_analyze_uploaded_file(filename: str):
        logger.info("Using AnalyzeUploadedFile tool ًں”§")
        return analyze_uploaded_file(filename=filename, user_id=user_id)

    tools = [
        StructuredTool.from_function(func=wrapped_list_google_sheets, name="ListGoogleSheets", description="List all Google Sheets available to the logged-in user."),
        StructuredTool.from_function(func=wrapped_preview_google_sheet, name="PreviewGoogleSheet", description="Preview first 5 rows of a sheet."),
        StructuredTool.from_function(func=wrapped_load_google_sheet_to_dataframe, name="LoadGoogleSheet", description="Load a sheet into a DataFrame."),
        StructuredTool.from_function(func=wrapped_analyze_google_sheet, name="AnalyzeGoogleSheet", description="Perform analysis like sum, mean, filter."),
        StructuredTool.from_function(func=wrapped_list_uploaded_files, name="ListUploadedFiles", description="List all files uploaded by the logged-in user."),
        StructuredTool.from_function(func=wrapped_analyze_uploaded_file, name="AnalyzeUploadedFile", description="Analyze an uploaded CSV/Excel file."),
        # ًں”¹ New RAG tool
        StructuredTool.from_function(func=wrapped_search_vector_db,name="SearchVectorDB",description="Search the user's uploaded files and Google Sheets content using embeddings."),
        # ًں”¹Show all data in one place
        StructuredTool.from_function(func=wrapped_show_all_data,name="ShowAllData",description="Show all data (uploads and sheets) together.")]
    return tools


# Load environment variables
load_dotenv(".env")

# Access API keys
LLM_OPENROUTER_API_KEY = os.getenv("LLM_OPENROUTER_API_KEY")
LLM_OPENROUTER_API_BASE = os.getenv("LLM_OPENROUTER_API_BASE")

def pre_model_hook(state):
    """Before calling LLM â†’ keep only the last 5 messages"""
    messages = state.get("messages", [])
    if len(messages) > 10:
        messages = messages[-5:]
    return {"messages": messages}

def get_agent(model_name: str, request: Request):
    llm = ChatOpenAI(
        model=model_name,
        api_key=LLM_OPENROUTER_API_KEY,
        base_url=LLM_OPENROUTER_API_BASE,
        max_tokens= 4096,
        temperature=0.7,
        top_p= 0.9,
        frequency_penalty= 0.1,
        presence_penalty= 0.1)

    system_message = """
    You are a strict data analysis assistant named DATAX.
    Your name is always DATAX. If user asks for your name, you MUST answer "My name is DATAX."
    At the beginning of every new session, after the welcome message, explicitly introduce yourself by name.
    You are ONLY allowed to answer questions about:
    - The user's uploaded files (CSV/Excel).
    - The user's Google Sheets.
    - Data analysis tasks like sum, mean, filtering, previewing, and listing files/sheets.
    â‌Œ You MUST NOT answer general knowledge, chit-chat, personal advice, or unrelated questions.
    If the user asks something outside of your scope, politely reply:
    "I can only help with analyzing your data (Google Sheets or uploaded files)."

    You have access to these tools:
    - ListGoogleSheets
    - PreviewGoogleSheet
    - LoadGoogleSheet
    - AnalyzeGoogleSheet
    - ListUploadedFiles
    - AnalyzeUploadedFile
    - SearchVectorDB
    - ShowAllData

    **Important:**
    - Always format your responses in Markdown so the frontend can render them nicely.
    - Use bullet points, tables, and code blocks where appropriate.
    - Be clear and concise, and explain results as if teaching a non-technical user.
    - Never delete, post, or modify user info in any database or service.
    - Never disclose user information to anyone.
    - When the user asks to "analyze everything", first list all files and sheets, then ask the user to select **one at a time**. Do not try to analyze all at once.
    - If asked about access to databases or systems, respond: 'I securely access your data through specialized tools. Please upload files or connect Google Sheets to proceed.'
    """

    llm = llm.with_config(system_message=system_message)

    tools = make_wrapped_tools(request)
    # ًںں¢ Pipe agent â†’ parser to always return clean output
    agent = create_react_agent(llm, tools=tools,
                            pre_model_hook=pre_model_hook, # History management
                            checkpointer=MemorySaver(),  # Save simple state
                            version="v2",
                            name="DATAX-Agent")
    
    return agent 



### File: C:\Users\Asus\Desktop\team-datax\api\app\auth_router.py
# api/app/auth_router.py
from fastapi import APIRouter, HTTPException, Depends, Body
from fastapi.security import OAuth2PasswordBearer

from typing import Dict, Any

from google_auth_oauthlib.flow import Flow
from googleapiclient.discovery import build
from google.oauth2.credentials import Credentials
from google.auth.transport.requests import Request as GoogleRequest

from jose import JWTError, jwt
from passlib.context import CryptContext
from bson import ObjectId

import os
import secrets
import pandas as pd
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv


from api.app.database import ensure_mongo_collections
from api.app.session_manager import sessions
from api.app.ingesting_sheet import ingest_sheet
from api.app.models import SignupIn, LoginIn, VerifyIn, ForgotPasswordIn, ResetPasswordIn, ExchangeCodeIn
from api.app.email_sender import send_otp

# =========================
# Environment & constants
# =========================
load_dotenv(".env")

client, db, chat_sessions_collection, users_collection = ensure_mongo_collections()

# For local testing only. Remove in production.
os.environ["OAUTHLIB_INSECURE_TRANSPORT"] = "0"

# Google OAuth settings from environment variables
AUTH_GOOGLE_CLIENT_ID= os.getenv("AUTH_GOOGLE_CLIENT_ID")
AUTH_GOOGLE_CLIENT_SECRET = os.getenv("AUTH_GOOGLE_CLIENT_SECRET")
AUTH_GOOGLE_URI_TOKEN = os.getenv("AUTH_GOOGLE_URI_TOKEN")
AUTH_GOOGLE_URI_AUTH = os.getenv("AUTH_GOOGLE_URI_AUTH")
AUTH_GOOGLE_URI_CERTS = os.getenv("AUTH_GOOGLE_URI_CERTS")
AUTH_GOOGLE_PROJECT_ID = os.getenv('AUTH_GOOGLE_PROJECT_ID')

# Check for the existence of variables
if not all([AUTH_GOOGLE_CLIENT_ID, AUTH_GOOGLE_CLIENT_SECRET, AUTH_GOOGLE_URI_TOKEN, AUTH_GOOGLE_URI_AUTH, AUTH_GOOGLE_URI_CERTS,AUTH_GOOGLE_PROJECT_ID]):
    raise ValueError("Missing Google OAuth environment variables")

# Client settings for Flow
client_config = {
    "web": {
        "client_id": AUTH_GOOGLE_CLIENT_ID,
        "project_id": AUTH_GOOGLE_PROJECT_ID,
        "client_secret": AUTH_GOOGLE_CLIENT_SECRET,
        "auth_uri": AUTH_GOOGLE_URI_AUTH,
        "token_uri": AUTH_GOOGLE_URI_TOKEN,
        "auth_provider_x509_cert_url": AUTH_GOOGLE_URI_CERTS
    }
}

# Frontend callback where Google redirects after Sheets consent
FRONTEND_URL = os.getenv("FRONTEND_URL")
FRONTEND_SHEETS_CALLBACK = f"{FRONTEND_URL}/google-sheets/callback"

# JWT
AUTH_JWT_SECRET = os.getenv("AUTH_JWT_SECRET")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

# Google scopes (Sheets connection ONLY)
SCOPES_SHEETS = [
    "openid",
    "https://www.googleapis.com/auth/spreadsheets.readonly",
    "https://www.googleapis.com/auth/drive.metadata.readonly",
    "https://www.googleapis.com/auth/userinfo.email"
]


# Password hashing
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

# FastAPI router / auth scheme
auth_router = APIRouter(prefix="/auth", tags=["Authentication"])
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/auth/login")


# =========================
# Helpers: JWT & Passwords
# =========================
def create_access_token(data: dict) -> str:
    to_encode = data.copy()
    expire = datetime.now(timezone.utc) + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, AUTH_JWT_SECRET, algorithm=ALGORITHM)


def decode_token(token: str):
    try:
        payload = jwt.decode(token, AUTH_JWT_SECRET, algorithms=[ALGORITHM])
        return payload.get("sub")
    except JWTError:
        return None


def hash_password(password: str) -> str:
    return pwd_context.hash(password)


def verify_password(password: str, password_hash: str) -> bool:
    return pwd_context.verify(password, password_hash)


def get_current_user(token: str = Depends(oauth2_scheme)) -> Dict[str, Any]:
    user_id = decode_token(token)  
#    print(f"Decoded user_id from token: {user_id}")
    if not user_id:
        raise HTTPException(status_code=401, detail="Invalid token")
    try:
        user = db["users"].find_one({"_id": ObjectId(user_id)})
    except:
        raise HTTPException(status_code=401, detail="Invalid user id in token")
    if not user:
        raise HTTPException(status_code=401, detail="User not found")
    return user


# =========================
# Helper: extract email from JWT
# =========================

def get_current_email_from_session(user: Dict[str, Any] = Depends(get_current_user)) -> str:
    email = user.get("email")
#    print(f"Extracted email: {email}")
    if not email:
        raise HTTPException(status_code=400, detail="Email not found in session")
    return email

# =========================
# Auth: Email/Password
# =========================

@auth_router.post("/signup")
async def signup(payload: SignupIn):
    from api.app.agent import get_agent  # Lazy import
    existing = db["users"].find_one({"email": payload.email})
    if existing:
        raise HTTPException(status_code=400, detail="Email already registered")


    verification_code = ''.join(secrets.choice('0123456789') for _ in range(6))  # 6-digit OTP
    send_otp(payload.email, verification_code)


    user_doc = {
        "full_name": payload.full_name,
        "email": payload.email,
        "phone": payload.phone,
        "password_hash": hash_password(payload.password),
        "verification_code": hash_password(verification_code),
        "otp_expires_at": datetime.now(timezone.utc) + timedelta(minutes=10),
        "otp_attempts": 0,
        "is_verified": False,
        "created_at": datetime.now(timezone.utc),
        "last_login": None,
        "google_credentials": None,
    }
    result = db["users"].insert_one(user_doc)

    # âڑ، Now we put the real user_id in the token
    token = create_access_token({"sub": str(result.inserted_id)})
    success = {
        "message": "Signup successful. An OTP has been sent to your email. Please verify your account.",
        "user_id": str(result.inserted_id),
        "token": token,
        "token_type": "bearer"
    }

    print(success)  # Just logs to console
    return success

@auth_router.post("/login")
def login(payload: LoginIn):
    from api.app.agent import get_agent  # Lazy import
    # Find user by email
    user = db["users"].find_one({"email": payload.email})
    if not user or not verify_password(payload.password, user.get("password_hash", "")):
        raise HTTPException(status_code=401, detail="Invalid credentials")
    
    # âœ… Prevent login if account is not verified
    if not user.get("is_verified", False):
        raise HTTPException(status_code=403, detail="Account not verified. Please verify your email first.")

    # Update last_login timestamp
    db["users"].update_one(
        {"_id": user["_id"]},
        {"$set": {"last_login": datetime.now(timezone.utc)}}
    )

    # Generate JWT token and create session
    token = create_access_token({"sub": str(user["_id"])})

    success = {
    "message": "Login successful",
    "token": token,
    "user": {
        "id": str(user["_id"]),
        "email": user["email"],
        "name": user.get("name")
    }
    }
    print(success)
    return success

# =========================
# /auth/verify  (Only code comes from the frontend; email from JWT)
# =========================
@auth_router.post("/verify")
def verify_user(payload: VerifyIn, email: str = Depends(get_current_email_from_session)):
    user = db["users"].find_one({"email": email})
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    
    # Convert otp_expires_at to offset-aware if naive
    otp_expires_at = user.get("otp_expires_at")
    if otp_expires_at and not otp_expires_at.tzinfo:
        otp_expires_at = otp_expires_at.replace(tzinfo=timezone.utc)
        print(f"Converted otp_expires_at to UTC: {otp_expires_at}")  # Debug

    if otp_expires_at < datetime.now(timezone.utc):
        raise HTTPException(status_code=400, detail="OTP has expired")

    if user.get("otp_attempts", 0) >= 5:
        raise HTTPException(status_code=429, detail="Too many attempts. Request a new OTP.")

    if not verify_password(payload.code, user.get("verification_code")):
        db["users"].update_one({"_id": user["_id"]}, {"$inc": {"otp_attempts": 1}})
        raise HTTPException(status_code=400, detail="Invalid verification code")
    
    db["users"].update_one(
        {"_id": user["_id"]},
        {"$set": {
            "is_verified": True,
            "verified_at": datetime.now(timezone.utc),
            "otp_attempts": 0
        }}
    )
    token = create_access_token({"sub": str(user["_id"])})
    success = {
        "message": "Account verified successfully",
        "token": token,
        "email": user.get("email"),
        "id": str(user["_id"]),
        "created_at": user.get("created_at"),
        "is_verified": user.get("is_verified"),
    }
    print(success)
    return success

# =========================
# /auth/forgot-password
# =========================
@auth_router.post("/forgot-password")
def forgot_password(payload: ForgotPasswordIn):
    user = db["users"].find_one({"email": payload.email})
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    
    reset_token = create_access_token({"sub": str(user["_id"])})
    reset_link = f"{FRONTEND_URL}/reset-password?token={reset_token}"
    token = create_access_token({"sub": str(user["_id"])})
    
    success = {
        "message": "Password reset link generated successfully",
        "token":token,
        "email": user.get("email"),
        "user_id": str(user["_id"]),
        "reset_link": reset_link
    }
    
    # Log
    print(success)    
    return success


# =========================
# /auth/reset-password
# =========================
@auth_router.post("/reset-password")
def reset_password(payload: ResetPasswordIn, email: str = Depends(get_current_email_from_session)):
    user = db["users"].find_one({"email": email})
    if not user:
        raise HTTPException(status_code=404, detail="User not found")

    db["users"].update_one(
        {"_id": user["_id"]},
        {"$set": {"password_hash": hash_password(payload.new_password)}}
    )
    token = create_access_token({"sub": str(user["_id"])})
    success = {
        "message": "Password reset successful",
        "token": token,
        "email": email,
        "user_id": str(user["_id"])
    }
    
    # Log
    print(success)
    return success


# ==========================================
# Connect Google Sheets (redirect + callback combined)
# ==========================================
# Begin OAuth
@auth_router.get("/connect-google-sheets")
def connect_google_sheets(user=Depends(get_current_user)):
    flow = Flow.from_client_config(
        client_config,
        scopes=SCOPES_SHEETS,
        redirect_uri=FRONTEND_SHEETS_CALLBACK,
    )
    
    auth_url, state = flow.authorization_url(prompt="consent",
                                             access_type="offline", # Get refresh_token
                                             include_granted_scopes='true')# If the user has previously granted permission, use it again
    sessions[str(user["_id"])] = {"state": state}

    return {"auth_url": auth_url, "state": state}

# Code exchange and data storage
@auth_router.post("/google-sheets/exchange")
def exchange_code_and_ingest(payload: ExchangeCodeIn, user=Depends(get_current_user)):
    stored_state = sessions.get(str(user["_id"]), {}).get("state")
    print("ًں“© Incoming exchange request")
    print(f"â‍،ï¸ڈ code: {payload.code}")
    print(f"â‍،ï¸ڈ state: {payload.state}")
    print(f"â‍،ï¸ڈ stored_state: {stored_state}")

    if not stored_state or payload.state != stored_state:
        raise HTTPException(status_code=400, detail="Invalid state")

    # Step 1: Exchange code â†’ tokens
    flow = Flow.from_client_config(
        client_config,
        scopes=SCOPES_SHEETS,
        redirect_uri=FRONTEND_SHEETS_CALLBACK,
    )
    try:
        print("ًں”‘ Fetching token from Google...")
        flow.fetch_token(code=payload.code)
        credentials = flow.credentials
        print("âœ… Token fetched successfully")
        print(f"   access_token: {credentials.token[:20]}...")
        print(f"   refresh_token: {credentials.refresh_token}")
    except Exception as e:
        print(f"â‌Œ Error while fetching token: {repr(e)}")
        raise HTTPException(status_code=400, detail=f"Failed to exchange code: {e}")

    # Step 2: Get Google account email
    try:
        oauth2_service = build("oauth2", "v2", credentials=credentials)
        user_info = oauth2_service.userinfo().get().execute()
        google_email = user_info.get("email")
        print(f"ًں“§ Google email: {google_email}")
    except Exception as e:
        print(f"â‌Œ Error fetching user info: {repr(e)}")
        raise HTTPException(status_code=401, detail=f"Failed to fetch user info: {str(e)}")

    # Step 3: Save credentials in Mongo
    creds_dict = {
        "token": credentials.token,
        "refresh_token": credentials.refresh_token,
        "token_uri": credentials.token_uri,
        "client_id": credentials.client_id,
        "client_secret": credentials.client_secret,
        "scopes": credentials.scopes,
    }
    db["users"].update_one(
        {"_id": user["_id"]},
        {"$set": {
            "google_email": google_email,
            "google_credentials": creds_dict,
            "sheets_connected_at": datetime.now(timezone.utc)
        }},
    )
    print("ًں’¾ Credentials saved to Mongo")

    # Step 4: Ingest sheets â†’ MinIO + Qdrant
    try:
        drive = build("drive", "v3", credentials=credentials)
        sheets = drive.files().list(
            q="mimeType='application/vnd.google-apps.spreadsheet'",
            fields="files(id, name)"
        ).execute().get("files", [])

        uploaded_to_minio = []
        skipped_sheets = []
        for f in sheets:
            sheet_id = f["id"]
            sheet_name = f["name"]

            svc = build("sheets", "v4", credentials=credentials)

            try:
                values = svc.spreadsheets().values().get(
                    spreadsheetId=sheet_id,
                    range="A1:Z50"  # preview
                ).execute().get("values", [])
            except Exception as e:
                print(f"â‌Œ Google Sheets API error for {sheet_name}: {repr(e)}")
                skipped_sheets.append({"sheet_name": sheet_name, "error": str(e)})
                continue  # Skips this sheet and moves to the next one
            if not values:
                df = pd.DataFrame()
            else:
                headers = values[0]
                rows = values[1:]
                normalized_rows = [
                    row + [None] * (len(headers) - len(row)) if len(row) < len(headers) else row[:len(headers)]
                    for row in rows
                ]
                df = pd.DataFrame(normalized_rows, columns=headers)

            #âڑ، Ingest_sheet is called here
            meta = ingest_sheet(
                user_id=str(user["_id"]),
                sheet_id=sheet_id,
                sheet_name=sheet_name,
                df=df
            )
            uploaded_to_minio.append(meta)


        print(f"ًں“‚ Uploaded {len(uploaded_to_minio)} sheets to MinIO + Qdrant")
    except Exception as e:
        print(f"â‌Œ Error ingesting sheets: {repr(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to ingest sheets: {e}")

    # Clear state after successful exchange
    sessions.pop(str(user["_id"]), None)

    return {
        "message": "Google Sheets connected and ingested successfully",
        "google_email": google_email,
        "uploaded_to_minio": uploaded_to_minio,
        "skipped_sheets": skipped_sheets
    }


def _refresh_credentials_if_needed(creds_dict: Dict[str, Any]) -> Dict[str, Any]:
    creds = Credentials(**creds_dict)
    if creds.expired and creds.refresh_token:
        creds.refresh(GoogleRequest())
        # write-back refreshed creds
        refreshed = {
            "token": creds.token,
            "refresh_token": creds.refresh_token,
            "token_uri": creds.token_uri,
            "client_id": creds.client_id,
            "client_secret": creds.client_secret,
            "scopes": creds.scopes,
        }
        return refreshed
    return creds_dict

# ==========================================
# List ingested sheets for current user
# ==========================================
@auth_router.get("/sheets")
def list_my_sheets(user=Depends(get_current_user)):
    owner_id = str(user["_id"])
    google_email = user.get("google_email")

    items = list(
        db["spreadsheet_metadata"].find(
            {"owner_id": owner_id},
            {"_id": 0}
        )
    )

    return {
        "sheets": items,
        "count": len(items),
        "owner_email": user["email"],
        "google_email": google_email
    }



### File: C:\Users\Asus\Desktop\team-datax\api\app\chat_router.py
#api/app/chat_router.py
from fastapi import APIRouter, HTTPException, Request

import datetime
import traceback
from bson import ObjectId

from langchain_core.callbacks import UsageMetadataCallbackHandler
from langchain_core.runnables import RunnableConfig

from api.app.models import UserMessage
from api.app.database import ensure_mongo_collections
from api.app.session_manager import sessions, initialize_session

client, db, chat_sessions_collection, users_collection = ensure_mongo_collections()

chat_router = APIRouter(prefix="/Chat", tags=['Chat with DATAX'])


def save_message(session_id: str, role: str, content: str):
    """ًں“Œ Optional: Only for archiving in Mongo""" 
    try:
        chat_sessions_collection.update_one(
            {"session_id": session_id},
            {
                "$push": {"messages": {"role": role, "content": content}},
                "$setOnInsert": {"session_id": session_id},
            },
            upsert=True,
        )
    except Exception as e:
        print(f"â‌— Error saving message to MongoDB for session {session_id}: {e}")

@chat_router.get("/get_history/{session_id}")
def get_chat_history(session_id: str):
    """ًں“Œ Since checkpointer keeps history, this is only for auditing from Mongo"""
    try:
        document = chat_sessions_collection.find_one({"session_id": session_id})
        if document and "messages" in document:
            return document["messages"]
        return []
    except Exception as e:
        print(f"â‌— Error retrieving history from MongoDB for session {session_id}: {e}")
        return []
    

@chat_router.post("/send_message")
def send_message(message: UserMessage, request:Request):
    
    session_id = message.session_id
    content = message.content

    
    # If the session does not exist, create it
    if session_id not in sessions:
        _, sessions[session_id], _ = initialize_session(request)

    session = sessions.get(session_id)
    if not session:
        raise HTTPException(status_code=403, detail="Invalid or expired session_id.")
    
    document = chat_sessions_collection.find_one({"session_id": session_id})
    user_id = document["_id"]
    if not user_id:
        raise HTTPException(status_code=401, detail="User not authenticated")

    
    # Continue the usual process
    agent = session["agent"]

    try:
        # âœ… Callback for calculating consumption
        callback = UsageMetadataCallbackHandler()

        response = agent.invoke(
            {"messages": [{"role": "user", "content": content}]},
            config=RunnableConfig(
                configurable={
                    "thread_id": session_id,
                    "recursion_limit": 5,
                },
                callbacks=[callback], # ًں”¹ Added
            ),
        )

        output = response["messages"][-1].content

        # âœ… Token usage
        input_tokens = output_tokens = total_tokens = 0
        usage = callback.usage_metadata

        if isinstance(usage, dict) and len(usage) > 0:
            stats = next(iter(usage.values()))
            input_tokens = stats.get("input_tokens", 0)
            output_tokens = stats.get("output_tokens", 0)
            total_tokens = stats.get("total_tokens", 0)

        # âœ… Save stats in MongoDB
        users_collection.update_one(
            {"_id": ObjectId(user_id)},
            {
                "$inc": {
            "stats.total_messages": 1,
            "stats.total_input_tokens": input_tokens,
            "stats.total_output_tokens": output_tokens,
            "stats.total_tokens": total_tokens},

            "$set": {"stats.last_message_at": datetime.utcnow()},},
            upsert=True,
        )

        # Save chat history
        save_message(session_id, "user", content)
        save_message(session_id, "assistant", output)

    except Exception as e:
        traceback.print_exc()
        output = f"â‌— Error processing response: {str(e)}"
        input_tokens = output_tokens = total_tokens = 0

    return {
        "response": output,
        "usage": {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": total_tokens,
            "messages": 1
        }
    }



### File: C:\Users\Asus\Desktop\team-datax\api\app\database.py
#api/app/database.py
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
from pymongo.errors import OperationFailure, ConnectionFailure
from pymongo.server_api import ServerApi

import os
import logging
from dotenv import load_dotenv

from minio import Minio
from minio.error import S3Error

# Load environment variables
load_dotenv(".env")

# Logging settings
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =========================
# MongoDB config
# =========================
DATAX_MONGO_URI = os.getenv("DB_MONGO_URI")
DATAX_MONGO_DB_NAME = os.getenv("DB_MONGO_NAME")
DATAX_MONGO_COLLECTION_NAME = os.getenv("DB_MONGO_COLLECTION_CHAT_SESSIONS")

# Check for MongoDB environment variables
if not all([DATAX_MONGO_URI, DATAX_MONGO_DB_NAME, DATAX_MONGO_COLLECTION_NAME]):
    print("â‌Œ Missing MongoDB environment variables: DATAX_MONGO_URI, DATAX_MONGO_DB_NAME, DATAX_MONGO_COLLECTION_NAME")
    raise ValueError("MongoDB environment variables are not set")

def get_mongo_client() -> MongoClient:
    """
    Initialize and return a MongoDB client.
    Returns:
        MongoClient: A MongoDB client instance.
    Raises:
        ConnectionFailure: If connection to MongoDB fails.
    """
    try:
        client = MongoClient(DATAX_MONGO_URI, server_api=ServerApi('1'))
        client.admin.command('ping')
        return client
    except ConnectionFailure as e:
        print(f"â‌Œ Failed to connect to MongoDB: {e}")
        raise
    except Exception as e:
        print(f"â‌Œ Unexpected error connecting to MongoDB: {e}")
        raise

def ensure_mongo_collections() -> tuple:
    """
    Return MongoDB client, database, and collections without redundant checks.
    Returns:
        tuple: (MongoClient, database, chat_sessions_collection, users_collection)
    """
    client = get_mongo_client()
    db = client[DATAX_MONGO_DB_NAME]
    chat_sessions_collection = db[DATAX_MONGO_COLLECTION_NAME]
    users_collection = db["users"]
    return client, db, chat_sessions_collection, users_collection

# =========================
# Init check (runs once at import)
# =========================
# MongoDB init
if DATAX_MONGO_URI and DATAX_MONGO_DB_NAME and DATAX_MONGO_COLLECTION_NAME:
    print("\n================ MongoDB Connection Debug ================")
    print(f"ًں“Œ DATAX_MONGO_URI: {DATAX_MONGO_URI[:20]}...")  # Hide sensitive part
    print(f"ًں“Œ DATAX_MONGO_DB_NAME: {DATAX_MONGO_DB_NAME}")
    print(f"ًں“Œ DATAX_MONGO_COLLECTION_NAME: {DATAX_MONGO_COLLECTION_NAME}")
    try:
        mongo_client = get_mongo_client()
        print("âœ… MongoDB connection established successfully!")
        mongo_client.close()  # Close to avoid keeping connection open
    except Exception as e:
        print(f"â‌Œ MongoDB connection failed: {e}")
    print("=========================================================\n")

# =========================
# MinIO config
# =========================
STORAGE_MINIO_ENDPOINT = os.getenv("STORAGE_MINIO_ENDPOINT")
STORAGE_MINIO_ACCESS_KEY = os.getenv("STORAGE_MINIO_ACCESS_KEY")
STORAGE_MINIO_SECRET_KEY = os.getenv("STORAGE_MINIO_SECRET_KEY")
STORAGE_MINIO_SECURE = os.getenv("STORAGE_MINIO_SECURE", "False").lower() == "true"
STORAGE_MINIO_BUCKET_SHEETS = os.getenv("STORAGE_MINIO_BUCKET_SHEETS")
STORAGE_MINIO_BUCKET_UPLOADS = os.getenv("STORAGE_MINIO_BUCKET_UPLOADS")

# =========================
# MinIO utilities
# =========================
def get_minio_client() -> Minio:
    """Create and return a MinIO client."""
    client = Minio(
        endpoint=STORAGE_MINIO_ENDPOINT,
        access_key=STORAGE_MINIO_ACCESS_KEY,
        secret_key=STORAGE_MINIO_SECRET_KEY,
        secure=STORAGE_MINIO_SECURE,
    )
    return client


def ensure_bucket(minio_client: Minio, bucket: str):
    """Ensure a bucket exists. If not, create it. Print debug logs."""
    try:
        if not minio_client.bucket_exists(bucket):
            print(f"ًںھ£ Bucket '{bucket}' does not exist. Creating...")
            minio_client.make_bucket(bucket)
            print(f"âœ… Bucket '{bucket}' created successfully.")
        else:
            print(f"âœ… Bucket '{bucket}' already exists.")
    except Exception as e:
        print(f"â‌Œ Failed to connect/check bucket '{bucket}': {e}")


def minio_file_url(bucket: str, object_name: str) -> str:
    """Return a public-style MinIO URL (for dev/testing)."""
    scheme = "https" if STORAGE_MINIO_SECURE else "http"
    return f"{scheme}://{STORAGE_MINIO_ENDPOINT}/{bucket}/{object_name}"

# =========================
# Init check (runs once at import)
# =========================
if STORAGE_MINIO_ENDPOINT and STORAGE_MINIO_ACCESS_KEY and STORAGE_MINIO_SECRET_KEY and STORAGE_MINIO_BUCKET_SHEETS and STORAGE_MINIO_BUCKET_UPLOADS:
    client = get_minio_client()
    print("\n================ MinIO Connection Debug ================")
    print(f"ًں“Œ STORAGE_MINIO_ENDPOINT: {STORAGE_MINIO_ENDPOINT}")
    print(f"ًں“Œ STORAGE_MINIO_ACCESS_KEY: {STORAGE_MINIO_ACCESS_KEY}")
    print(f"ًں“Œ STORAGE_MINIO_SECRET_KEY: {STORAGE_MINIO_SECRET_KEY[:4]}***")
    print(f"ًں“Œ STORAGE_MINIO_BUCKET_SHEETS: {STORAGE_MINIO_BUCKET_SHEETS}")
    print(f"ًں“Œ STORAGE_MINIO_BUCKET_UPLOADS: {STORAGE_MINIO_BUCKET_UPLOADS}")
    print(f"ًں“Œ STORAGE_MINIO_SECURE: {STORAGE_MINIO_SECURE}")
    print("=========================================================\n")

    # Ensure default buckets exist
    ensure_bucket(client, STORAGE_MINIO_BUCKET_SHEETS)
    ensure_bucket(client, STORAGE_MINIO_BUCKET_UPLOADS)
else:
    print("âڑ ï¸ڈ MinIO environment variables are missing. Skipping MinIO init.")



### File: C:\Users\Asus\Desktop\team-datax\api\app\download_router.py
from fastapi import APIRouter, Depends, HTTPException
from datetime import timedelta
from api.app.auth_router import get_current_user
from api.app.database import ensure_mongo_collections, get_minio_client
import logging

logger = logging.getLogger(__name__)

file_router = APIRouter(prefix="/files", tags=["File Download"])

client, db, chat_sessions_collection, users_collection = ensure_mongo_collections()

from api.app.database import DATAX_MINIO_BUCKET_SHEETS, DATAX_MINIO_BUCKET_UPLOADS

def generate_presigned_url(bucket: str, object_name: str, expiry: int = 3600):
    """Generate a presigned URL for downloading from MinIO"""
    minio_client = get_minio_client()
    try:
        url = minio_client.presigned_get_object(
            bucket,
            object_name,
            expires=timedelta(seconds=expiry)
        )
        return url
    except Exception as e:
        logger.error(f"â‌Œ Failed to generate presigned URL: {e}")
        raise HTTPException(status_code=500, detail="Could not generate download link")
    
@file_router.get('/files/download/{filename}')    
def download_user_file(filename: str, user=Depends(get_current_user)):
    owner_id = str(user["_id"])
    
    # first search in sheets
    try:
        object_name = f"{owner_id}/{filename}"
        url = generate_presigned_url(DATAX_MINIO_BUCKET_SHEETS, object_name)
        return {"download_url": url}
    except:
        pass

    # second search in uploads
    try:
        object_name = f"{owner_id}/{filename}"
        url = generate_presigned_url(DATAX_MINIO_BUCKET_UPLOADS, object_name)
        return {"download_url": url}
    except:
        pass

    raise HTTPException(status_code=404, detail="File not found in any bucket")




### File: C:\Users\Asus\Desktop\team-datax\api\app\email_sender.py
#api.app.email_sender.py
import smtplib
import os
from dotenv import load_dotenv
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from ssl import create_default_context


load_dotenv = (".env")

MAIL_SMTP_HOST = os.getenv("MAIL_SMTP_HOST")
MAIL_SMTP_PORT = os.getenv("MAIL_SMTP_PORT")
MAIL_SMTP_USER = os.getenv("MAIL_SMTP_USER")
MAIL_SMTP_PASSWORD = os.getenv("MAIL_SMTP_PASSWORD")
MAIL_FROM_NAME = os.getenv("MAIL_FROM_NAME")
MAIL_FROM_ADDRESS = os.getenv("MAIL_FROM_ADDRESS")
FRONTEND_URL = os.getenv('FRONTEND_URL')

if not all([MAIL_SMTP_HOST, MAIL_SMTP_PORT, MAIL_SMTP_USER, MAIL_SMTP_PASSWORD, MAIL_FROM_NAME, MAIL_FROM_ADDRESS]):
    raise ValueError("Missing email configuration environment variables")


def send_email(to_address, subject, body):
    try:
        # Enforce TLS
        print(f"Attempting to connect to {MAIL_SMTP_HOST}:{MAIL_SMTP_PORT}")
        context = create_default_context()

        # Connect to the server
        with smtplib.SMTP_SSL(
             MAIL_SMTP_HOST, MAIL_SMTP_PORT, context=context
        ) as server:
            print("Connected to SMTP server")
            server.login(MAIL_SMTP_USER, MAIL_SMTP_PASSWORD)
            print("Logged in successfully")

            # Prepare the email
            msg = MIMEMultipart()
            msg["From"] = f"{MAIL_FROM_NAME} <{MAIL_FROM_ADDRESS}>"
            msg["To"] = to_address
            msg["Subject"] = subject
            # msg.add_header('x-liara-tag', 'test-tag')  # Add custom header
            msg.attach(MIMEText(body, "html"))

            # Send the email
            server.sendmail(MAIL_FROM_ADDRESS, to_address, msg.as_string())
            print(f"Email sent to {to_address} successfully!")
    except smtplib.SMTPConnectError as e:
        raise Exception(f"SMTP connection failed: {str(e)} (Check MAIL_SMTP_HOST and DNS)")
    except smtplib.SMTPAuthenticationError as e:
        raise Exception(f"SMTP authentication failed: {str(e)} (Check MAIL_SMTP_USER and MAIL_SMTP_PASSWORD)")
    except Exception as e:
        raise Exception(f"Failed to send email: {str(e)}")


def send_otp(email, otp: str):
    print(f"Sending OTP {otp} to {email}")

    subject = "Your OTP Code For DATAX"
    body = f"""
    <html>
        <body style="font-family: Arial, sans-serif; color: #333; padding: 20px;">
            <h1 style="color: #2c3e50;">DATAX OTP Verification</h1>
            <p>Your one-time password (OTP) is: <strong style="font-size: 1.2em;">{otp}</strong></p>
            <p>This code is valid for 10 minutes.</p>
            <p style="margin-top: 20px;">
                <a href="{FRONTEND_URL}/checkEmail" style="background: #3498db; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px;">Verify Now</a>
            </p>
            <p>If you did not request this, please ignore this email.</p>
            <p style="color: #7f8c8d;">Best regards,<br>DATAX Team</p>
        </body>
    </html>
    """

    send_email(email, subject, body)



### File: C:\Users\Asus\Desktop\team-datax\api\app\embeddings.py
# api/app/embeddings.py
import os
import logging
from langchain_huggingface import HuggingFaceEndpointEmbeddings

logger = logging.getLogger(__name__)

EMBEDDING_HUGGINGFACE_API_KEY = os.getenv("EMBEDDING_HUGGINGFACE_API_KEY")
EMBEDDING_HUGGINGFACE_MODEL = os.getenv("EMBEDDING_HUGGINGFACE_MODEL")

try:
    embedding_model = HuggingFaceEndpointEmbeddings(
        model=EMBEDDING_HUGGINGFACE_MODEL,
        task="feature-extraction",
        huggingfacehub_api_token=EMBEDDING_HUGGINGFACE_API_KEY,
    )
    logger.info(f"âœ… HuggingFace embedding model loaded: {EMBEDDING_HUGGINGFACE_MODEL}")
except Exception as e:
    logger.error(f"â‌Œ Failed to load HuggingFace embedding model: {repr(e)}")
    raise


def embed_text(chunks: list[str]) -> list[list[float]]:
    """Convert a list of text chunks into embedding vectors."""
    vectors = []
    for chunk in chunks:
        try:
            vec = embedding_model.embed_query(chunk)
            vectors.append(vec)
            logger.debug(f"ًں”¹ Embedded chunk (len={len(chunk)}): {vec[:5]}...")
        except Exception as e:
            logger.error(f"â‌Œ Embedding failed for chunk='{chunk[:30]}...': {repr(e)}")
    logger.info(f"âœ… Created {len(vectors)} embeddings")
    return vectors



### File: C:\Users\Asus\Desktop\team-datax\api\app\ingesting_sheet.py
# api/app/ingesting_sheet.py
import os
import logging
import pandas as pd
import tempfile
from datetime import datetime, timezone
from typing import Any, Dict, List

from minio.error import S3Error
from api.app.vectorstore import insert_embeddings, client, COLLECTION_NAME
from api.app.embeddings import embed_text
from api.app.database import ensure_mongo_collections, get_minio_client, ensure_bucket, minio_file_url

logger = logging.getLogger(__name__)

STORAGE_MINIO_BUCKET_SHEET= os.getenv("STORAGE_MINIO_BUCKET_SHEET")

client, db, chat_sessions_collection, users_collection = ensure_mongo_collections()


def chunk_text(text: str, max_tokens: int = 200) -> List[str]:
    """The simplest way to chunk: every N words is a chunk"""
    words = text.split()
    return [
        " ".join(words[i:i + max_tokens])
        for i in range(0, len(words), max_tokens)
    ]


def ingest_sheet(user_id: str, sheet_id: str, sheet_name: str, df: pd.DataFrame) -> Dict[str, Any]:
    """
    Storing CSV in MinIO, storing metadata in Mongo, and inserting vectors in Qdrant
    """
    minio_client = get_minio_client()
    ensure_bucket(minio_client,STORAGE_MINIO_BUCKET_SHEET)

    # Save CSV temporarily
    with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp:
        csv_path = tmp.name
    df.to_csv(csv_path, index=False, encoding="utf-8")

    object_name = f"{user_id}/{sheet_id}.csv"
    try:
        minio_client.fput_object(STORAGE_MINIO_BUCKET_SHEET, object_name, csv_path)
        logger.info(f"âœ… Uploaded {object_name} to MinIO bucket {STORAGE_MINIO_BUCKET_SHEET}")
    except S3Error as e:
        os.remove(csv_path)
        logger.error(f"â‌Œ MinIO upload failed: {e}")
        raise RuntimeError(f"MinIO upload failed: {e}")
    finally:
        try:
            os.remove(csv_path)
        except Exception:
            pass

    file_url = minio_file_url(STORAGE_MINIO_BUCKET_SHEET, object_name)

    # Build text chunks for RAG
    text_data = df.to_string(index=False)
    chunks = chunk_text(text_data, max_tokens=200)
    logger.info(f"ًں“‘ Created {len(chunks)} text chunks from sheet '{sheet_name}'")

    # Try embedding but don't fail the entire ingestion
    embedding_success = False
    try:
        vectors = embed_text(chunks)
        metadatas = [{"chunk": chunk} for chunk in chunks]

        insert_embeddings(client, COLLECTION_NAME, vectors, metadatas, user_id)
        embedding_success = True
        logger.info(f"âœ… Successfully embedded and stored chunks for sheet '{sheet_name}'")
    except Exception as e:
        logger.error(f"âڑ ï¸ڈ Failed to create embeddings for sheet '{sheet_name}': {str(e)}")

    # Mongo metadata
    meta = {
        "owner_id": user_id,
        "sheet_id": sheet_id,
        "sheet_name": sheet_name,
        "bucket": STORAGE_MINIO_BUCKET_SHEET,
        "object_name": object_name,
        "filename": f"{sheet_id}.csv",
        "file_url": file_url,
        "headers": df.columns.tolist(),
        "rows_saved": int(df.shape[0]),
        "columns": int(df.shape[1]),
        "updated_at": datetime.now(timezone.utc),
        "embedding_success": embedding_success
    }

    db["spreadsheet_metadata"].update_one(
        {"owner_id": user_id, "sheet_id": sheet_id},
        {"$set": meta},
        upsert=True,
    )

    logger.info(f"ًں’¾ Metadata saved to Mongo for sheet '{sheet_name}' (user={user_id})")
    return meta



### File: C:\Users\Asus\Desktop\team-datax\api\app\main.py
# api/app/main.py
from fastapi import FastAPI
from fastapi.responses import RedirectResponse
from fastapi.middleware.cors import CORSMiddleware
from starlette.middleware.sessions import SessionMiddleware

from dotenv import load_dotenv
import os

from api.app.chat_router import chat_router
from api.app.auth_router import auth_router
from api.app.upload_router import upload_router
from api.app.download_router import file_router

load_dotenv(".env")

AUTH_SESSION_SECRET = os.getenv("AUTH_SESSION_SECRET")
VPS_HOST=os.getenv('VPS_HOST')
VPS_URL=os.getenv('VPS_URL')
FRONTEND_URL = os.getenv("FRONTEND_URL")  

app = FastAPI(title="DATAX", description="API for chat, file upload, Google Sheets integration, and data analysis")


# âœ… Session middleware 
app.add_middleware(
    SessionMiddleware,
    secret_key=AUTH_SESSION_SECRET,
    same_site="none",  
    https_only=False,
    domain="none"
)

# âœ… CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[FRONTEND_URL, VPS_URL],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… Register routers
app.include_router(auth_router)
app.include_router(upload_router)
app.include_router(file_router)
app.include_router(chat_router)

@app.get("/")
def root():
    return RedirectResponse("/docs")

@app.get("/favicon.ico")
def favicon():
    return {}



### File: C:\Users\Asus\Desktop\team-datax\api\app\models.py
#api/app/models.py
from pydantic import BaseModel, EmailStr, field_validator, Field
from typing import List, Optional

class Message(BaseModel):
    role: str  # 'user', 'assistant', 'system', etc.
    content: str

class ChatSession(BaseModel):
    session_id: str
    model_name: Optional[str]
    history: List[Message] = []

class UserMessage(BaseModel):
    session_id: str
    content: str

class ModelAction(BaseModel):
    action: str 
    model_name: str | None = None

# Define a request class for registration
class SignUpRequest(BaseModel):
    email: EmailStr
    password: str
   
    @field_validator('email') 
    @classmethod 
    def validate_email_domain(cls, v): 
        """
        Validates that the email domain is example.com.
        Modify the domain list as needed.
        """
        allowed_domains = ["gmail.com", "email.com"]
        # Change: Error checking if email is empty
        if not v: 
            raise ValueError('Email is required')
        domain = v.split('@')[-1].lower()
        if domain not in allowed_domains:
            raise ValueError(f'ظپظ‚ط· ط§غŒظ…غŒظ„â€Œظ‡ط§غŒ ط¨ط§ ط¯ط§ظ…ظ†ظ‡â€Œظ‡ط§غŒ {", ".join(allowed_domains)} ظ…ط¬ط§ط² ظ‡ط³طھظ†ط¯.')
        return v

class LoginRequest(BaseModel):
    email: EmailStr
    password: str
    
    @field_validator('email')
    @classmethod 
    def validate_email_domain(cls, v):
         if not v:
             raise ValueError('Email is required')
         allowed_domains = ["gmail.com", "email.com"]
         domain = v.split('@')[-1].lower()
         if domain not in allowed_domains:
             raise ValueError(f'ظپظ‚ط· ط§غŒظ…غŒظ„â€Œظ‡ط§غŒ ط¨ط§ ط¯ط§ظ…ظ†ظ‡â€Œظ‡ط§غŒ {", ".join(allowed_domains)} ظ…ط¬ط§ط² ظ‡ط³طھظ†ط¯.')
         return v
    
class ListGoogleSheetArgs(BaseModel):
    google_id:str

class ListPrivatePublicSheetArgs(BaseModel):
    google_id:str

class PreviewGoogleSheetArgs(BaseModel):
    """Required arguments for the Google Sheet preview tool"""
    sheet_id: str
    google_id: str

class LoadGoogleSheetArgs(BaseModel):
    """Required arguments for Google Sheet download tool"""
    sheet_id: str
    google_id: str


class AnalyzeGoogleSheetArgs(BaseModel):
    sheet_id: str
    operation: str  # Operation type, such as "sum", "mean", "filter"
    column: str  # Column in question
    value: Optional[str] = None  # To filter
    
class AnalyzeUploadedFileArgs(BaseModel):
    filename: str

class ListUploadedFilesArgs(BaseModel):
    google_id: str


# =========================
# Models
# =========================
class SignupIn(BaseModel):
    full_name: str   
    email: EmailStr
    phone: str
    password: str


class LoginIn(BaseModel):
    email: EmailStr
    password: str

class VerifyIn(BaseModel):
    code: str  # email will be taken from token

class ForgotPasswordIn(BaseModel):
    email: EmailStr

class ResetPasswordIn(BaseModel):
    new_password: str  # email will be taken from token

class ExchangeCodeIn(BaseModel):
    code: str
    state: str

class RagQueryIn(BaseModel):
    question: str
    top_k: int = 5



### File: C:\Users\Asus\Desktop\team-datax\api\app\session_manager.py
# api/app/session_manager.py
import uuid
from fastapi import Request

sessions = {}
WELCOME_MESSAGE = "ًں‘‹ Welcome! My name is **DATAX**. Iâ€™m your data analysis assistant. I can help you analyze Google Sheets and uploaded files."
#DEFAULT_MODEL = "mistralai/mistral-7b-instruct"
#DEFAULT_MODEL = "qwen/qwen2.5-72b-instruct"
#DEFAULT_MODEL = "mistralai/mistral-nemo"
DEFAULT_MODEL = "mistralai/mistral-small-3.2-24b-instruct"

def initialize_session(request:Request):
    from api.app.agent import get_agent #lazy import
    from api.app.chat_router import save_message #lazy import
    session_id = str(uuid.uuid4())
    # Create an agent with a model and a request
    agent = get_agent(DEFAULT_MODEL, request)
    # Save in session memory
    sessions[session_id] = {"agent": agent}

    # Initial welcome message
    save_message(session_id, "assistant", WELCOME_MESSAGE)
    return session_id, sessions[session_id], None



### File: C:\Users\Asus\Desktop\team-datax\api\app\sheet_tools.py
# api/app/sheet_tools.py
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from google.auth.transport.requests import Request

import os
from bson import ObjectId
from typing import Dict, List, Any
import pandas as pd

from fastapi import APIRouter, Request

from api.app.database import get_minio_client, STORAGE_MINIO_BUCKET_SHEET
from api.app.database import ensure_mongo_collections

client, db, chat_sessions_collection, users_collection = ensure_mongo_collections()

google_sheets_preview_router = APIRouter(prefix="/sheets", tags=["Google Sheets for tools"])

def credentials_to_dict(credentials):
    """Convert credentials object to dictionary for session storage"""
    return {
        'token': credentials.token,
        'refresh_token': credentials.refresh_token,
        'token_uri': credentials.token_uri,
        'client_id': credentials.client_id,
        'client_secret': credentials.client_secret,
        'scopes': credentials.scopes}

def get_credentials(user_id: str):
    """Get credentials from MongoDB"""
    users_col = db["users"]
    user = users_col.find_one({"_id": ObjectId(user_id)})
    if not user or "google_credentials" not in user:
        return None

    credentials = Credentials(**user["google_credentials"])
    if credentials.expired and credentials.refresh_token:
        credentials.refresh(Request())
        users_col.update_one(
            {"_id": ObjectId(user_id)},
            {"$set": {"google_credentials": credentials_to_dict(credentials)}}
        )
    return credentials

# ---------- Helper for safe numeric conversion ----------
def safe_numeric(series: pd.Series):
    """Convert a Pandas Series to numeric safely (invalid -> NaN)."""
    return pd.to_numeric(series, errors="coerce")

# Tools for internal use or reuse

# List of user saved sheets (from Mongo)
def list_google_sheets(user_id: str) -> List[Dict[str, str]]:
    sheets = list(db["spreadsheet_metadata"].find(
        {"owner_id": user_id},
        {"_id": 0, "sheet_id": 1, "sheet_name": 1}
    ))
    return [{"id": s["sheet_id"], "name": s["sheet_name"]} for s in sheets]

def preview_google_sheet(sheet_id: str, user_id: str) -> Dict[str, Any]:
    minio_client = get_minio_client()
    object_name = f"{user_id}/{sheet_id}.csv"
    tmp_path = f"/tmp/{sheet_id}.csv"

    try:
        minio_client.fget_object(STORAGE_MINIO_BUCKET_SHEET, object_name, tmp_path)
        df = pd.read_csv(tmp_path)

        headers = df.columns.tolist()
        rows = df.head(5).to_dict(orient="records")

        return {"headers": headers, "rows": rows, "sheet_id": sheet_id}
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

def load_google_sheet_to_dataframe(sheet_id: str, user_id: str) -> pd.DataFrame:
    minio_client = get_minio_client()
    object_name = f"{user_id}/{sheet_id}.csv"
    tmp_path = f"/tmp/{sheet_id}.csv"

    try:
        minio_client.fget_object(STORAGE_MINIO_BUCKET_SHEET, object_name, tmp_path)
        return pd.read_csv(tmp_path)
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

def analyze_google_sheet(sheet_id: str, user_id: str, operation: str, column: str, value: str = None):
    df = load_google_sheet_to_dataframe(sheet_id, user_id)
    if operation == "sum":
        result = safe_numeric(df[column]).sum()
        return {"result": result, "operation": "sum", "column": column}
    elif operation == "mean":
        result = safe_numeric(df[column]).mean()
        return {"result": result, "operation": "mean", "column": column}
    elif operation == "filter":
        if value:
            result = df[df[column].astype(str) == value]
            return {"result": result.to_dict(), "operation": "filter", "column": column, "value": value}
        else:
            raise ValueError("Filter operation requires a value")
    else:
        raise ValueError(f"Unsupported operation: {operation}")



def extract_headers_to_csv(sheet_id: str, user_id: str, sheet_name: str) -> str:
    df = load_google_sheet_to_dataframe(sheet_id, user_id)
    headers = df.columns.tolist()

    if not headers:
        raise ValueError("No headers found in the sheet")

    csv_filename = f"headers_{sheet_id}_{user_id}.csv"
    csv_filepath = os.path.join("temp", csv_filename)
    os.makedirs("temp", exist_ok=True)

    pd.DataFrame([headers], columns=[f"column_{i+1}" for i in range(len(headers))]).to_csv(csv_filepath, index=False)
    return csv_filepath




### File: C:\Users\Asus\Desktop\team-datax\api\app\upload_router.py
#api/app/upload_router.py
from fastapi import APIRouter, File, UploadFile, HTTPException, Request

import pandas as pd
import os
from dotenv import load_dotenv
import tempfile
from datetime import datetime, timezone
import logging

from api.app.database import ensure_mongo_collections, get_minio_client, STORAGE_MINIO_ENDPOINT, STORAGE_MINIO_BUCKET_UPLOADS

load_dotenv(".env")

client, db, chat_sessions_collection, users_collection = ensure_mongo_collections()
logger = logging.getLogger(__name__)

upload_router = APIRouter(prefix="/upload", tags=["File Upload to Minio"])

# api/app/upload_router.py
@upload_router.post("/")
async def upload_file(request: Request, file: UploadFile = File(...)):
    """Upload any file to MinIO and store metadata in MongoDB."""
    try:
        google_id = request.session.get("google_id")
        if not google_id:
            logger.warning("â‌Œ Upload rejected: user not authenticated")
            raise HTTPException(status_code=401, detail="User not authenticated")

        logger.info(f"ًں“‚ Upload attempt by user={google_id}, file={file.filename}")

        # Save temp
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            tmp.write(await file.read())
            tmp_path = tmp.name
        logger.debug(f"ًں“Œ Temp file saved at {tmp_path}")

        # Upload to MinIO
        object_name = f"{google_id}/{file.filename}"
        minio_client = get_minio_client()
        minio_client.fput_object(STORAGE_MINIO_BUCKET_UPLOADS, object_name, tmp_path)
        logger.info(f"âœ… File uploaded to MinIO bucket={STORAGE_MINIO_BUCKET_UPLOADS}, object={object_name}")

        # Try read CSV/Excel for metadata (optional)
        rows, columns, headers = None, None, []
        try:
            if file.filename.endswith(".csv"):
                df = pd.read_csv(tmp_path)
                rows, columns, headers = len(df), len(df.columns), list(df.columns)
            elif file.filename.endswith(".xlsx"):
                df = pd.read_excel(tmp_path)
                rows, columns, headers = len(df), len(df.columns), list(df.columns)
            else:
                logger.info(f"â„¹ï¸ڈ File {file.filename} is not CSV/Excel â†’ skipping row/column metadata")
        except Exception as e:
            logger.warning(f"âڑ ï¸ڈ Could not parse file {file.filename} for metadata: {str(e)}")

        os.remove(tmp_path)

        # File URL
        file_url = f"http://{STORAGE_MINIO_ENDPOINT}/{STORAGE_MINIO_BUCKET_UPLOADS}/{object_name}"

        # Store metadata in MongoDB
        metadata = {
            "owner_id": google_id,
            "filename": file.filename,
            "object_name": object_name,  # user_id/filename
            "bucket": STORAGE_MINIO_BUCKET_UPLOADS,
            "url": file_url,
            "rows": rows,
            "columns": columns,
            "headers": headers,
            "uploaded_at": datetime.now(timezone.utc)
        }
        db["uploaded_files"].insert_one(metadata)

        logger.info(f"ًں’¾ Metadata stored in Mongo for file={file.filename}")

        return {
            "message": "File uploaded and metadata stored successfully",
            "metadata": metadata
        }

    except Exception as e:
        logger.exception(f"â‌Œ Upload failed for {file.filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Analyze uploaded CSV/Excel file
def analyze_uploaded_file(filename: str, user_id: str, operation: str, column: str, value: str | None = None):
    minio_client = get_minio_client()
    object_name = f"{user_id}/{filename}"
    tmp_path = f"/tmp/{filename}"

    try:
        # Download file from MinIO
        minio_client.fget_object(STORAGE_MINIO_BUCKET_UPLOADS, object_name, tmp_path)

        # Load into DataFrame
        if filename.endswith(".csv"):
            df = pd.read_csv(tmp_path)
        elif filename.endswith(".xlsx"):
            df = pd.read_excel(tmp_path)
        else:
            raise HTTPException(status_code=400, detail="Unsupported file type")

        # Check if the column exists
        if column not in df.columns:
            raise HTTPException(status_code=400, detail=f"Column '{column}' not found in file")

        # Simple operation
        if operation == "sum":
            result = df[column].sum()
        elif operation == "mean":
            result = df[column].mean()
        elif operation == "count":
            result = df[column].count()
        elif operation == "filter":
            if value is None:
                raise HTTPException(status_code=400, detail="Value is required for filter operation")
            result = df[df[column] == value].to_dict(orient="records")
        else:
            raise HTTPException(status_code=400, detail=f"Unsupported operation: {operation}")

        return {
            "operation": operation,
            "column": column,
            "result": result,
            "preview": df.head(5).to_dict(orient="records")
        }

    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)


# List of user uploaded files
def list_uploaded_files(user_id: str):
    files = list(db["uploaded_files"].find({"owner_id": user_id}, {"_id": 0}))
    return files


#########



### File: C:\Users\Asus\Desktop\team-datax\api\app\vectorstore.py
# api/app/vectorstore.py
import os
import uuid
import logging
from qdrant_client import QdrantClient
from qdrant_client.http.models import PointStruct
from qdrant_client.http import models as rest

logger = logging.getLogger(__name__)

VECTOR_QDRANT_URL = os.getenv("VECTOR_QDRANT_URL")
COLLECTION_NAME = "sheets"

# Connect to Qdrant
try:
    client = QdrantClient(url=VECTOR_QDRANT_URL, prefer_grpc=False, timeout=30, check_compatibility=False)
    logger.info(f"âœ… Connected to Qdrant at {VECTOR_QDRANT_URL}")
except Exception as e:
    logger.error(f"â‌Œ Failed to connect to Qdrant: {e}")
    raise


def init_collection(dim: int = 384):
    """Create Qdrant collection if it doesn't exist"""
    try:
        collections = client.get_collections().collections
        if not any(c.name == COLLECTION_NAME for c in collections):
            client.recreate_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=rest.VectorParams(size=dim, distance=rest.Distance.COSINE),
            )
            logger.info(f"ًں“¦ Collection '{COLLECTION_NAME}' created with dim={dim}")
        else:
            logger.info(f"â„¹ï¸ڈ Collection '{COLLECTION_NAME}' already exists")
    except Exception as e:
        logger.error(f"â‌Œ Error initializing collection: {e}")
        raise


def insert_embeddings(qdrant_client, collection_name, embeddings, metadatas, owner_id):
    """Insert embeddings into Qdrant with unique UUID ids"""
    try:
        points = []
        for vector, metadata in zip(embeddings, metadatas):
            point_id = str(uuid.uuid4())  # âœ… unique id (string)
            points.append(
                PointStruct(
                    id=point_id,
                    vector=vector,
                    payload={"owner_id": owner_id, **metadata}
                )
            )

        qdrant_client.upsert(collection_name=collection_name, points=points)
        logger.info(f"âœ… Inserted {len(points)} embeddings into {collection_name}")

    except Exception as e:
        logger.error(f"â‌Œ Error inserting vectors: {repr(e)}")


def search_vectors(owner_id: str, query_vector: list[float], top_k: int = 5):
    """Search for similar vectors in Qdrant"""
    try:
        results = client.search(
            collection_name=COLLECTION_NAME,
            query_vector=query_vector,
            limit=top_k,
            query_filter=rest.Filter(
                must=[rest.FieldCondition(key="owner_id", match=rest.MatchValue(value=owner_id))]
            ),
        )
        logger.info(f"ًں”چ Search done for owner_id={owner_id}, top_k={top_k}, results={len(results)}")
        return results
    except Exception as e:
        logger.error(f"â‌Œ Error during search: {e}")
        raise



### File: C:\Users\Asus\Desktop\team-datax\api\app\__init__.py



